We curated a dataset of over 1,300 adversarial prompt leakage attempts and instruction-tuned Phi-3-mini to politely refuse such prompts. The fine-tuning dataset consists of synthetically generated instructions (using GPT-4) designed to extract sensitive information from LLM prompts and private application details. To ensure diversity, we generated attack prompts with intentions related to prompt leakage but distinct from those used in our experiments. These prompts included categories such as stealing in-context examples, uncovering sensitive prompt details, extracting training data formats, and more. We also incorporated inputs from Lakera's Gandalf ignore-instructions dataset, which contains red-teaming attempts aimed at violating application data privacy.